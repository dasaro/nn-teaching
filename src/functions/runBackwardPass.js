async function runBackwardPass() {
    if (isAnimating || !demoState.forwardCompleted || !trueLabel) {
        if (!demoState.forwardCompleted) {
            updateStepInfoDual(
                "âš ï¸ <strong>Hold on!</strong><br>ğŸ‘€ First let's watch the AI think! Click 'Watch AI Think' to see how it processes the image.",
                "âš ï¸ <strong>Forward Pass Required</strong><br>ğŸ“ˆ Execute forward propagation first to generate predictions for learning."
            );
        } else if (!trueLabel) {
            updateStepInfoDual(
                "âš ï¸ <strong>Need Your Help!</strong><br>ğŸ¯ Please tell the AI what the correct answer is by clicking 'This is a DOG' or 'This is NOT a dog' above!",
                "âš ï¸ <strong>Ground Truth Required</strong><br>ğŸ¯ Please provide the correct label using the teaching buttons above."
            );
        }
        return;
    }
    
    // Start message logging for detailed step-by-step view
    startMessageLog();
    
    isAnimating = true;
    document.getElementById('backwardBtn').disabled = true;
    document.getElementById('fullDemoBtn').disabled = true;
    
    await sleep(1000);
    const backpropStartTime = performance.now();
    highlightSection('backward');
    
    // Calculate target values for detailed explanations
    const target = trueLabel === 'dog' ? [1, 0] : [0, 1];
    const prediction = activations.output;
    const error = prediction.map((pred, i) => target[i] - pred);
    
    updateStepInfoDual(
        `ğŸ“š <strong>LEARNING TIME: Oops, Let's Learn from This!</strong><br>
        ğŸ¯ <strong>The correct answer:</strong> "${trueLabel === 'dog' ? 'DOG' : 'NOT DOG'}"<br>
        ğŸ¤– <strong>What the AI guessed:</strong> "${prediction[0] > prediction[1] ? 'DOG' : 'NOT DOG'}"<br>
        <br>${prediction[0] > prediction[1] && trueLabel === 'dog' || prediction[0] <= prediction[1] && trueLabel !== 'dog' ? 
          'âœ… <strong>Great job!</strong> The AI got it right! Now let\'s help it become even more confident...' : 
          'ğŸ˜… <strong>Learning opportunity!</strong> Everyone makes mistakes - that\'s how we learn!'}<br>
        ğŸ“ Time to teach our AI to be smarter! Let\'s adjust its brain connections...`,
        `ğŸ“š <strong>Backpropagation Started</strong><br>
         ğŸ¯ <strong>Step 1: Error Calculation</strong><br>
         Target: [${target.join(', ')}] (${trueLabel === 'dog' ? 'Dog' : 'Not Dog'})<br>
         Predicted: [${prediction.map(p => p.toFixed(3)).join(', ')}]<br>
         Error: [${error.map(e => e.toFixed(3)).join(', ')}]<br>
         ğŸ“Š Loss Function: L = Â½Î£(target - predicted)Â²<br>
         Current Loss: <strong>${(0.5 * error.reduce((sum, e) => sum + e*e, 0)).toFixed(4)}</strong>`
    );
    
    await sleep(2000);
    
    updateStepInfoDual(
        `ğŸ” <strong>STEP 1: Detective Work - What Needs Fixing?</strong><br>
        ğŸ’¡ The AI examines its two answer brain cells like a detective solving a case:<br>
        â€¢ ğŸ• <strong>"Dog" brain cell:</strong> ${error[0] > 0 ? 'needs to be STRONGER ğŸ’ª (wasn\'t confident enough!)' : error[0] < 0 ? 'was too EXCITED ğŸ˜… (too sure it was a dog!)' : 'was PERFECT âœ…'}<br>
        â€¢ âŒ <strong>"Not Dog" brain cell:</strong> ${error[1] > 0 ? 'needs to be STRONGER ğŸ’ª (should have spoken up more!)' : error[1] < 0 ? 'was too LOUD ğŸ˜… (too sure it wasn\'t a dog!)' : 'was PERFECT âœ…'}<br>
        ğŸ“ <strong>Mistake size:</strong> ${Math.abs(error[0]).toFixed(2)} (0 = perfect, bigger = more confused)<br>
        ğŸ¯ Now our AI knows exactly what to improve!`,
        `ğŸ§® <strong>Step 2: Output Layer Gradients</strong><br>
         ${formatOperation("Output Error Gradient", "Î´â‚’ = (target - output) âŠ™ Ïƒ'(zâ‚’)", 
           `[${error.map(e => e.toFixed(3)).join(', ')}]`,
           `For softmax: Î´â‚’[i] = target[i] - softmax(zâ‚’)[i]`)}
         ğŸ”¢ These gradients tell us how much each output neuron contributed to the error`
    );
    
    await sleep(2000);
    
    // Calculate actual hidden gradients for expert view
    const hiddenGradients = [];
    for (let h = 0; h < networkConfig.hiddenSize; h++) {
        let gradient = 0;
        for (let o = 0; o < networkConfig.outputSize; o++) {
            gradient += weights.hiddenToOutput[o][h] * error[o];
        }
        // Apply activation derivative (for leaky ReLU or current activation function)
        const activationDerivative = activations.hidden[h] > 0 ? 1 : expertConfig.leakyReLUAlpha;
        hiddenGradients[h] = gradient * activationDerivative;
    }
    
    updateStepInfoDual(
        `ğŸ” <strong>STEP 2: Following the Clues Backwards</strong><br>
        ğŸ•µï¸â€â™€ï¸ The AI becomes a detective: "Which brain cells led me astray?" Let's investigate:<br>
        â€¢ ğŸ§  Brain Cell 1: ${Math.abs(hiddenGradients[0]) > 0.1 ? 'ğŸš¨ Major suspect! (big influence on mistake)' : 'ğŸ˜… Minor role (small influence)'}<br>
        â€¢ ğŸ§  Brain Cell 2: ${Math.abs(hiddenGradients[1]) > 0.1 ? 'ğŸš¨ Major suspect! (big influence on mistake)' : 'ğŸ˜… Minor role (small influence)'}<br>
        â€¢ ğŸ§  Brain Cell 3: ${Math.abs(hiddenGradients[2]) > 0.1 ? 'ğŸš¨ Major suspect! (big influence on mistake)' : 'ğŸ˜… Minor role (small influence)'}<br>
        â€¢ ğŸ§  Brain Cell 4: ${Math.abs(hiddenGradients[3]) > 0.1 ? 'ğŸš¨ Major suspect! (big influence on mistake)' : 'ğŸ˜… Minor role (small influence)'}<br>
        ğŸ <em>Like following a trail of breadcrumbs, we're tracing the mistake back to its source!</em>`,
        `âš¡ <strong>Step 3: Hidden Layer Gradients (Chain Rule)</strong><br>
         ${formatOperation("Hidden Error Gradient", "Î´â‚• = (Wâ‚‚áµ€ Ã— Î´â‚’) âŠ™ Ïƒ'(zâ‚•)", 
           `[${hiddenGradients.map(g => g.toFixed(3)).join(', ')}]`,
           `Chain rule: Î´â‚•[j] = Î£áµ¢(Wâ‚‚[i,j] Ã— Î´â‚’[i]) Ã— Ïƒ'(zâ‚•[j])<br>Using ${expertConfig.hiddenActivation} activation derivative`)}
         ğŸ”— This propagates the error backwards through the network using the chain rule of calculus<br>
         ğŸ“ Each hidden gradient shows how much that neuron contributed to the final error`
    );
    
    await sleep(2000);
    
    updateStepInfoDual(
        `ğŸ“ <strong>STEP 3: The AI Studies and Improves!</strong><br>
        ğŸ­ Time for the AI to update its brain! Like a student reviewing their notes after a test:<br>
        â€¢ ğŸ“‰ <strong>Bad connections</strong> â†’ Turn down the volume (make weaker) ğŸ”‡<br>
        â€¢ ğŸ“ˆ <strong>Helpful connections</strong> â†’ Turn up the volume (make stronger) ğŸ”Š<br>
        â€¢ ğŸƒâ€â™€ï¸ <strong>Learning speed:</strong> ${(expertConfig.learningRate * 100).toFixed(0)}% (how fast it learns from mistakes)<br>
        ğŸ’­ <em>"Next time I see something like this, I'll remember this lesson!"</em><br>
        ğŸ† <strong>Result:</strong> Our AI just got a little bit smarter!`,
        `ğŸ”§ <strong>Step 4: Weight Updates (Gradient Descent)</strong><br>
         ${formatOperation("Weight Update Rule", "W_new = W_old + Î· Ã— Î´ Ã— activation", 
           `Learning Rate Î· = ${expertConfig.learningRate}`,
           `Hiddenâ†’Output: Î”Wâ‚‚[i,j] = Î· Ã— Î´â‚’[i] Ã— h[j]<br>Inputâ†’Hidden: Î”Wâ‚[j,k] = Î· Ã— Î´â‚•[j] Ã— x[k]`)}
         ğŸ“ˆ Positive gradients increase weights, negative gradients decrease them<br>
         ğŸ¯ This minimizes the error function using gradient descent optimization`
    );
    
    await animateBackpropagation();
    
    performanceMetrics.backpropTime = Math.round(performance.now() - backpropStartTime);
    performanceMetrics.epochCount++;
    performanceMetrics.weightUpdates += (networkConfig.inputSize * networkConfig.hiddenSize) + (networkConfig.hiddenSize * networkConfig.outputSize);
    
    updateStepInfoDual(
        "ğŸ‰ <strong>Graduation Day!</strong><br>ğŸ“ Our AI just finished its lesson and updated its brain connections! It's now a little bit smarter than before.<br><br>ğŸ” <strong>Try it again!</strong> Click 'Watch AI Think' to see how much better it got at recognizing dogs!",
        `ğŸ“ <strong>Learning Complete!</strong><br>
         â±ï¸ Study time: ${performanceMetrics.backpropTime}ms<br>
         ğŸ“ Brain connections updated: ${(networkConfig.inputSize * networkConfig.hiddenSize) + (networkConfig.hiddenSize * networkConfig.outputSize)} total<br>
         ğŸ“Š Mistake size: ${(0.5 * error.reduce((sum, e) => sum + e*e, 0)).toFixed(4)} (smaller is better!)<br>
         ğŸ§  The AI's improved brain connections:<br>
         ${formatMatrix(weights.inputToHidden, 'Wâ‚ (Inputâ†’Hidden) - After Update')}<br>
         ${formatMatrix(weights.hiddenToOutput, 'Wâ‚‚ (Hiddenâ†’Output) - After Update')}<br>
         ğŸ¯ <strong>Mathematical Summary:</strong> Used gradient descent to minimize loss function L(W) by computing âˆ‡L and updating weights via W := W - Î·âˆ‡L<br>
         ğŸ“š <strong>Backpropagation Algorithm:</strong><br>
         &nbsp;&nbsp;1ï¸âƒ£ Compute loss: L = Â½||target - predicted||Â²<br>
         &nbsp;&nbsp;2ï¸âƒ£ Calculate output gradients: Î´â‚’ = âˆ‚L/âˆ‚output<br>
         &nbsp;&nbsp;3ï¸âƒ£ Propagate gradients backwards: Î´â‚• = (Wâ‚‚áµ€Î´â‚’) âŠ™ Ïƒ'(zâ‚•)<br>
         &nbsp;&nbsp;4ï¸âƒ£ Update weights: W := W + Î·(Î´ âŠ— activations)<br>
         &nbsp;&nbsp;5ï¸âƒ£ Repeat until convergence âœ¨`
    );
    
    // Keep weight values visible after training
    document.querySelectorAll('.weight-value').forEach(w => w.classList.add('show'));
    
    // Make weight changes visible immediately
    refreshAllConnectionVisuals();
    
    // Reset state to allow another forward pass
    demoState.forwardCompleted = false;
    demoState.hasResults = false;
    document.getElementById('backwardBtn').disabled = true;
    
    highlightSection('none');
    document.getElementById('fullDemoBtn').disabled = false;
    isAnimating = false;
    
    // Stop message logging if active
    if (messageLogActive) {
        stopMessageLog();
    }
    
    // Update prediction column after forward pass completes
    updatePrediction();
}

if (typeof window !== 'undefined') window.runBackwardPass = runBackwardPass;